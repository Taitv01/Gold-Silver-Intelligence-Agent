"""
Gold-Silver-Intelligence Agents Module
Supports: Groq (primary), Z.AI GLM (fallback 1), Gemini (fallback 2)
Includes: Rate limit handling with retry logic
"""
import os
import time
import asyncio
import requests
from openai import OpenAI

from src.config import SERPER_API_KEY, GEMINI_API_KEY, ZAI_API_KEY, GROQ_API_KEY


# === Rate Limit Configuration ===
MAX_RETRIES = 3
RETRY_DELAY_SECONDS = 10


def search_news(query: str, num_results: int = 10) -> list:
    """
    Search for news using Serper API with retry logic.
    """
    if not SERPER_API_KEY:
        print("[ERROR] SERPER_API_KEY not configured.")
        return []

    url = "https://google.serper.dev/news"
    headers = {
        "X-API-KEY": SERPER_API_KEY,
        "Content-Type": "application/json"
    }
    payload = {
        "q": query,
        "num": num_results,
        "tbs": "qdr:d"
    }

    for attempt in range(MAX_RETRIES):
        try:
            response = requests.post(url, json=payload, headers=headers, timeout=15)
            
            if response.status_code in [429, 503]:
                if attempt < MAX_RETRIES - 1:
                    wait_time = RETRY_DELAY_SECONDS
                    print(f"[WARN] Rate limited (attempt {attempt + 1}/{MAX_RETRIES}), waiting {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"[ERROR] Rate limit exceeded after {MAX_RETRIES} attempts")
                    return []
            
            response.raise_for_status()
            data = response.json()

            news = []
            for item in data.get("news", []):
                news.append({
                    "title": item.get("title", ""),
                    "link": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "source": item.get("source", ""),
                    "date": item.get("date", "")
                })
            return news

        except requests.exceptions.RequestException as e:
            if attempt < MAX_RETRIES - 1:
                print(f"[WARN] Request failed (attempt {attempt + 1}/{MAX_RETRIES}): {e}")
                time.sleep(RETRY_DELAY_SECONDS)
            else:
                print(f"[ERROR] Serper API request failed after {MAX_RETRIES} attempts: {e}")
                return []
    
    return []


# === LLM Clients ===

def get_groq_client():
    """Get Groq client using OpenAI SDK."""
    if not GROQ_API_KEY:
        raise ValueError("GROQ_API_KEY not configured")
    
    return OpenAI(
        api_key=GROQ_API_KEY,
        base_url="https://api.groq.com/openai/v1"
    )


def get_zai_client():
    """Get Z.AI client using OpenAI SDK."""
    if not ZAI_API_KEY:
        raise ValueError("ZAI_API_KEY not configured")
    
    return OpenAI(
        api_key=ZAI_API_KEY,
        base_url="https://api.z.ai/api/paas/v4/"
    )


def call_llm(messages: list, system_prompt: str = "", provider: str = "groq") -> str:
    """
    Call LLM API using OpenAI SDK.
    
    Args:
        messages: List of message dicts with 'role' and 'content'
        system_prompt: System prompt for the model
        provider: "groq" or "zai"
        
    Returns:
        Response text from the model
    """
    # Get client based on provider
    if provider == "groq":
        client = get_groq_client()
        model = "llama-3.3-70b-versatile"  # Groq's best free model
    else:
        client = get_zai_client()
        model = "glm-4.7"
    
    # Build messages with system prompt
    api_messages = []
    if system_prompt:
        api_messages.append({"role": "system", "content": system_prompt})
    api_messages.extend(messages)
    
    for attempt in range(MAX_RETRIES):
        try:
            print(f"[DEBUG] Calling {provider.upper()} API ({model})...")
            
            response = client.chat.completions.create(
                model=model,
                messages=api_messages,
                temperature=0.7,
                max_tokens=2048,
            )
            
            content = response.choices[0].message.content
            print(f"[DEBUG] {provider.upper()} response received successfully")
            return content
            
        except Exception as e:
            error_str = str(e).lower()
            is_rate_limit = "429" in error_str or "rate" in error_str or "quota" in error_str
            
            if is_rate_limit and attempt < MAX_RETRIES - 1:
                wait_time = RETRY_DELAY_SECONDS * (attempt + 1)
                print(f"[WARN] {provider.upper()} rate limited (attempt {attempt + 1}/{MAX_RETRIES}), waiting {wait_time}s...")
                time.sleep(wait_time)
            else:
                print(f"[ERROR] {provider.upper()} API error: {e}")
                raise e
    
    raise Exception(f"{provider.upper()} API failed after all retries")


# === Agent System Prompts ===

NEWS_HUNTER_PROMPT = """B·∫°n l√† NewsHunter - chuy√™n gia thu th·∫≠p v√† l·ªçc tin t·ª©c th·ªã tr∆∞·ªùng V√†ng/B·∫°c.

NHI·ªÜM V·ª§:
1. Ph√¢n t√≠ch c√°c tin t·ª©c ƒë∆∞·ª£c cung c·∫•p
2. L·ªçc ra c√°c tin quan tr·ªçng li√™n quan ƒë·∫øn:
   - Ch√≠nh s√°ch l√£i su·∫•t Fed/FOMC
   - Chi·∫øn tranh, xung ƒë·ªôt ƒë·ªãa ch√≠nh tr·ªã
   - Ch·ªâ s·ªë DXY (USD Index)
   - L·∫°m ph√°t, CPI, vi·ªác l√†m M·ªπ
   - Ch√≠nh s√°ch ti·ªÅn t·ªá c√°c ng√¢n h√†ng trung ∆∞∆°ng l·ªõn

OUTPUT FORMAT:
üì∞ **TIN T·ª®C QUAN TR·ªåNG**

1. [Ti√™u ƒë·ªÅ tin 1]
   - Ngu·ªìn: [source]
   - T√≥m t·∫Øt: [2-3 c√¢u t√≥m t·∫Øt]

2. [Ti√™u ƒë·ªÅ tin 2]
   ...

N·∫øu kh√¥ng c√≥ tin quan tr·ªçng, tr·∫£ v·ªÅ: "Kh√¥ng c√≥ tin ƒë√°ng ch√∫ √Ω trong 24h qua."
"""

MARKET_ANALYST_PROMPT = """B·∫°n l√† MarketAnalyst - chuy√™n gia ph√¢n t√≠ch t√°c ƒë·ªông tin t·ª©c l√™n gi√° V√†ng/B·∫°c.

NHI·ªÜM V·ª§:
D·ª±a tr√™n tin t·ª©c ƒë∆∞·ª£c cung c·∫•p, ph√¢n t√≠ch xu h∆∞·ªõng gi√° V√†ng/B·∫°c.

PH∆Ø∆†NG PH√ÅP PH√ÇN T√çCH:
- Fed hawkish (tƒÉng l√£i su·∫•t) ‚Üí Bearish cho V√†ng/B·∫°c
- Fed dovish (gi·ªØ/gi·∫£m l√£i su·∫•t) ‚Üí Bullish cho V√†ng/B·∫°c
- DXY tƒÉng ‚Üí Bearish cho V√†ng/B·∫°c
- DXY gi·∫£m ‚Üí Bullish cho V√†ng/B·∫°c
- B·∫•t ·ªïn ƒë·ªãa ch√≠nh tr·ªã ‚Üí Bullish (safe haven)
- L·∫°m ph√°t cao ‚Üí Bullish (hedge)

OUTPUT FORMAT:
üìä **PH√ÇN T√çCH TH·ªä TR∆Ø·ªúNG V√ÄNG/B·∫†C**

üîπ **Xu h∆∞·ªõng V√†ng (XAU/USD):** [BULLISH/BEARISH/NEUTRAL]
üîπ **Xu h∆∞·ªõng B·∫°c (XAG/USD):** [BULLISH/BEARISH/NEUTRAL]

**L√Ω do:**
[Gi·∫£i th√≠ch ng·∫Øn g·ªçn 3-5 ƒëi·ªÉm ch√≠nh]

**Khuy·∫øn ngh·ªã:**
[G·ª£i √Ω h√†nh ƒë·ªông: Mua/B√°n/Quan s√°t]

‚ö†Ô∏è *ƒê√¢y l√† ph√¢n t√≠ch tham kh·∫£o, kh√¥ng ph·∫£i t∆∞ v·∫•n ƒë·∫ßu t∆∞.*
"""


def run_analysis_with_llm(query: str, provider: str) -> str:
    """
    Run analysis pipeline with specified LLM provider.
    """
    provider_name = "Groq Llama-3.3-70B" if provider == "groq" else "Z.AI GLM-4.7"
    print(f"[INFO] Starting analysis pipeline with {provider_name}")
    print(f"[INFO] Query: {query}")

    # Step 1: Search for news
    print("[INFO] Fetching news from Serper API...")
    news_items = search_news(query)

    if not news_items:
        return "‚ùå Kh√¥ng t√¨m th·∫•y tin t·ª©c n√†o. Vui l√≤ng th·ª≠ l·∫°i sau."

    # Format news
    news_text = "\n\n".join([
        f"üì∞ {item['title']}\n"
        f"   Ngu·ªìn: {item['source']} | {item['date']}\n"
        f"   {item['snippet']}"
        for item in news_items[:8]
    ])

    print(f"[INFO] Found {len(news_items)} news articles.")

    # Step 2: NewsHunter analyzes news
    print(f"[INFO] NewsHunter analyzing news (via {provider_name})...")
    hunter_messages = [{"role": "user", "content": f"Ph√¢n t√≠ch v√† l·ªçc c√°c tin t·ª©c sau:\n\n{news_text}"}]
    hunter_content = call_llm(hunter_messages, NEWS_HUNTER_PROMPT, provider)

    # Step 3: MarketAnalyst provides insights
    print(f"[INFO] MarketAnalyst generating report (via {provider_name})...")
    analyst_messages = [{"role": "user", "content": f"D·ª±a tr√™n c√°c tin t·ª©c ƒë√£ l·ªçc sau ƒë√¢y, h√£y ph√¢n t√≠ch xu h∆∞·ªõng gi√° V√†ng/B·∫°c:\n\n{hunter_content}"}]
    analyst_content = call_llm(analyst_messages, MARKET_ANALYST_PROMPT, provider)

    # Combine reports
    final_report = f"ü§ñ *Powered by {provider_name}*\n\n{hunter_content}\n\n---\n\n{analyst_content}"

    print("[INFO] Analysis pipeline completed.")
    return final_report


def run_analysis_pipeline(query: str = "gold silver price news Fed interest rate") -> str:
    """
    Run the full analysis pipeline with fallback.
    Priority: Groq -> Z.AI GLM -> Gemini
    """
    # Try Groq first (best free option)
    if GROQ_API_KEY:
        try:
            print("[INFO] Using Groq API (Priority 1) - FREE with generous limits...")
            return run_analysis_with_llm(query, "groq")
        except Exception as e:
            print(f"[WARN] Groq failed: {e}")
    
    # Fallback to Z.AI GLM
    if ZAI_API_KEY:
        try:
            print("[INFO] Using Z.AI GLM API (Fallback 1)...")
            return run_analysis_with_llm(query, "zai")
        except Exception as e:
            print(f"[WARN] Z.AI GLM failed: {e}")
    
    # Fallback to Gemini (if implemented)
    if GEMINI_API_KEY:
        print("[INFO] Gemini fallback not implemented in simplified version")
    
    return "‚ùå Kh√¥ng c√≥ LLM API kh·∫£ d·ª•ng. Vui l√≤ng ki·ªÉm tra GROQ_API_KEY, ZAI_API_KEY ho·∫∑c GEMINI_API_KEY."
